{"cells":[{"cell_type":"markdown","id":"50337152-fd2d-4ed8-a9e9-0163b105d778","metadata":{"execution":{"iopub.execute_input":"2025-01-26T07:29:55.082304Z","iopub.status.busy":"2025-01-26T07:29:55.081977Z","iopub.status.idle":"2025-01-26T07:29:55.088135Z","shell.execute_reply":"2025-01-26T07:29:55.086590Z","shell.execute_reply.started":"2025-01-26T07:29:55.082280Z"},"id":"50337152-fd2d-4ed8-a9e9-0163b105d778"},"source":["# Text Processing Techniques\n"]},{"cell_type":"code","execution_count":2,"id":"941c5f4f-56d7-4b5f-8a36-644a13d1a0ce","metadata":{"execution":{"iopub.execute_input":"2025-01-27T03:15:34.769781Z","iopub.status.busy":"2025-01-27T03:15:34.769391Z","iopub.status.idle":"2025-01-27T03:15:41.845092Z","shell.execute_reply":"2025-01-27T03:15:41.843986Z","shell.execute_reply.started":"2025-01-27T03:15:34.769754Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"941c5f4f-56d7-4b5f-8a36-644a13d1a0ce","executionInfo":{"status":"ok","timestamp":1737948347651,"user_tz":420,"elapsed":10750,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"cb609e42-67e5-4135-beaf-cd320b17ad26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["#!pip install spacy\n","!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":9,"id":"d5459d0c-e9bf-46c2-8689-5dc52591a21c","metadata":{"execution":{"iopub.execute_input":"2025-01-27T03:18:01.689098Z","iopub.status.busy":"2025-01-27T03:18:01.684865Z","iopub.status.idle":"2025-01-27T03:18:04.127973Z","shell.execute_reply":"2025-01-27T03:18:04.117825Z","shell.execute_reply.started":"2025-01-27T03:18:01.689045Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"d5459d0c-e9bf-46c2-8689-5dc52591a21c","executionInfo":{"status":"ok","timestamp":1737948553289,"user_tz":420,"elapsed":1251,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"369a6a38-ed92-4cbd-b119-1f2cdb928918"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 14882 entries, 0 to 14881\n","Data columns (total 9 columns):\n"," #   Column       Non-Null Count  Dtype  \n","---  ------       --------------  -----  \n"," 0   review_id    14882 non-null  object \n"," 1   user_id      14882 non-null  object \n"," 2   business_id  14882 non-null  object \n"," 3   stars        14881 non-null  float64\n"," 4   useful       14881 non-null  float64\n"," 5   funny        14881 non-null  float64\n"," 6   cool         14881 non-null  float64\n"," 7   text         14881 non-null  object \n"," 8   date         14881 non-null  object \n","dtypes: float64(4), object(5)\n","memory usage: 1.0+ MB\n","None\n","                review_id                 user_id             business_id  \\\n","0  IVS7do_HBzroiCiymNdxDg  fdFgZQQYQJeEAshH4lxSfQ  sGy67CpJctjeCWClWqonjA   \n","1  QP2pSzSqpJTMWOCuUuyXkQ  JBLWSXBTKFvJYYiM-FnCOQ  3w7NRntdQ9h0KwDsksIt5Q   \n","2  oK0cGYStgDOusZKz9B1qug  2_9fKnXChUjC5xArfF8BLg  OMnPtRGmbY8qH_wIILfYKA   \n","3  E_ABvFCNVLbfOgRg3Pv1KQ  9MExTQ76GSKhxSWnTS901g  V9XlikTxq0My4gE8LULsjw   \n","4  Rd222CrrnXkXukR2iWj69g  LPxuausjvDN88uPr-Q4cQA  CA5BOxKRDPGJgdUQ8OUOpw   \n","\n","   stars  useful  funny  cool  \\\n","0    3.0     1.0    1.0   0.0   \n","1    5.0     1.0    1.0   1.0   \n","2    5.0     1.0    0.0   0.0   \n","3    5.0     0.0    0.0   0.0   \n","4    4.0     1.0    0.0   0.0   \n","\n","                                                text                 date  \n","0  OK, the hype about having Hatch chili in your ...  2020-01-27 22:59:06  \n","1  Pandemic pit stop to have an ice cream.... onl...  2020-04-19 05:33:16  \n","2  I was lucky enough to go to the soft opening a...  2020-02-29 19:43:44  \n","3  I've gone to claim Jumpers all over the US and...  2020-03-14 21:47:07  \n","4  If you haven't been  to Maynard's kitchen, it'...  2020-01-17 20:32:57  \n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import spacy\n","from collections import Counter\n","\n","# Load spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Load the dataset\n","data = pd.read_csv('restaurant_reviews_az.csv')\n","\n","# Display the summary of the dataset\n","print(data.info())\n","print(data.head())"]},{"cell_type":"code","execution_count":10,"id":"4f36b6bb-3cdd-4559-91d3-b8e46b8dd454","metadata":{"execution":{"iopub.execute_input":"2025-01-27T03:18:19.162867Z","iopub.status.busy":"2025-01-27T03:18:19.162417Z","iopub.status.idle":"2025-01-27T03:18:19.178377Z","shell.execute_reply":"2025-01-27T03:18:19.177456Z","shell.execute_reply.started":"2025-01-27T03:18:19.162839Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"4f36b6bb-3cdd-4559-91d3-b8e46b8dd454","executionInfo":{"status":"ok","timestamp":1737948559075,"user_tz":420,"elapsed":171,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"cc807d72-938b-4210-e971-c2513765d9b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["1-star reviews: 2454\n","5-star reviews: 7747\n"]}],"source":["# Filter for 1-star and 5-star reviews\n","one_star_reviews = data[data['stars'] == 1]\n","five_star_reviews = data[data['stars'] == 5]\n","\n","# Combine into a single DataFrame for further processing\n","selected_reviews = pd.concat([one_star_reviews, five_star_reviews])\n","\n","# Display basic statistics\n","print(\"1-star reviews:\", one_star_reviews.shape[0])\n","print(\"5-star reviews:\", five_star_reviews.shape[0])"]},{"cell_type":"code","execution_count":11,"id":"a42b3e88-6747-4267-a8a2-13365b4cc085","metadata":{"execution":{"iopub.execute_input":"2025-01-27T03:18:41.484151Z","iopub.status.busy":"2025-01-27T03:18:41.483678Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"a42b3e88-6747-4267-a8a2-13365b4cc085","executionInfo":{"status":"ok","timestamp":1737948796461,"user_tz":420,"elapsed":235465,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"7db91913-5500-400b-f96d-3ccf3de38532"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-38a8984aba68>:30: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  one_star_reviews['processed'] = one_star_reviews['text'].apply(lambda x: nlp(x))\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- 1-STAR REVIEWS ---\n","\n","Original Text: I stay at the Main Hotel at the Casino from July 11 to July 13; it was the WORST experience I've ever had.  For years I have supported this hotel and the casino; however, this time...it was a disaster. Will I go back, hopefullly, NO!\n","Sentences: [\"I stay at the Main Hotel at the Casino from July 11 to July 13; it was the WORST experience I've ever had.  \", 'For years I have supported this hotel and the casino; however, this time...', 'it was a disaster.', 'Will I go back, hopefullly, NO!']\n","Tokens: ['I', 'stay', 'at', 'the', 'Main', 'Hotel', 'at', 'the', 'Casino', 'from', 'July', '11', 'to', 'July', '13', ';', 'it', 'was', 'the', 'WORST', 'experience', 'I', \"'ve\", 'ever', 'had', '.', ' ', 'For', 'years', 'I', 'have', 'supported', 'this', 'hotel', 'and', 'the', 'casino', ';', 'however', ',', 'this', 'time', '...', 'it', 'was', 'a', 'disaster', '.', 'Will', 'I', 'go', 'back', ',', 'hopefullly', ',', 'NO', '!']\n","POS Tags: [('I', 'PRON'), ('stay', 'VERB'), ('at', 'ADP'), ('the', 'DET'), ('Main', 'PROPN'), ('Hotel', 'PROPN'), ('at', 'ADP'), ('the', 'DET'), ('Casino', 'PROPN'), ('from', 'ADP'), ('July', 'PROPN'), ('11', 'NUM'), ('to', 'ADP'), ('July', 'PROPN'), ('13', 'NUM'), (';', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('the', 'DET'), ('WORST', 'ADJ'), ('experience', 'NOUN'), ('I', 'PRON'), (\"'ve\", 'AUX'), ('ever', 'ADV'), ('had', 'VERB'), ('.', 'PUNCT'), (' ', 'SPACE'), ('For', 'ADP'), ('years', 'NOUN'), ('I', 'PRON'), ('have', 'AUX'), ('supported', 'VERB'), ('this', 'DET'), ('hotel', 'NOUN'), ('and', 'CCONJ'), ('the', 'DET'), ('casino', 'NOUN'), (';', 'PUNCT'), ('however', 'ADV'), (',', 'PUNCT'), ('this', 'DET'), ('time', 'NOUN'), ('...', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('a', 'DET'), ('disaster', 'NOUN'), ('.', 'PUNCT'), ('Will', 'AUX'), ('I', 'PRON'), ('go', 'VERB'), ('back', 'ADV'), (',', 'PUNCT'), ('hopefullly', 'ADV'), (',', 'PUNCT'), ('NO', 'PROPN'), ('!', 'PUNCT')]\n","Lemmatized Tokens: ['I', 'stay', 'at', 'the', 'Main', 'Hotel', 'at', 'the', 'Casino', 'from', 'July', '11', 'to', 'July', '13', ';', 'it', 'be', 'the', 'bad', 'experience', 'I', 'have', 'ever', 'have', '.', ' ', 'for', 'year', 'I', 'have', 'support', 'this', 'hotel', 'and', 'the', 'casino', ';', 'however', ',', 'this', 'time', '...', 'it', 'be', 'a', 'disaster', '.', 'will', 'I', 'go', 'back', ',', 'hopefullly', ',', 'NO', '!']\n","Named Entities: [('the Main Hotel', 'FAC'), ('Casino', 'ORG'), ('July 11 to July 13', 'DATE'), ('WORST', 'ORG')]\n","Dependency Parsing: [('I', 'nsubj', 'stay'), ('stay', 'ccomp', 'was'), ('at', 'prep', 'stay'), ('the', 'det', 'Hotel'), ('Main', 'compound', 'Hotel'), ('Hotel', 'pobj', 'at'), ('at', 'prep', 'Hotel'), ('the', 'det', 'Casino'), ('Casino', 'pobj', 'at'), ('from', 'prep', 'stay'), ('July', 'pobj', 'from'), ('11', 'nummod', 'July'), ('to', 'prep', 'from'), ('July', 'pobj', 'to'), ('13', 'nummod', 'July'), (';', 'punct', 'was'), ('it', 'nsubj', 'was'), ('was', 'ROOT', 'was'), ('the', 'det', 'experience'), ('WORST', 'amod', 'experience'), ('experience', 'attr', 'was'), ('I', 'nsubj', 'had'), (\"'ve\", 'aux', 'had'), ('ever', 'advmod', 'had'), ('had', 'relcl', 'experience'), ('.', 'punct', 'was'), (' ', 'dep', '.'), ('For', 'prep', 'supported'), ('years', 'pobj', 'For'), ('I', 'nsubj', 'supported'), ('have', 'aux', 'supported'), ('supported', 'ROOT', 'supported'), ('this', 'det', 'hotel'), ('hotel', 'dobj', 'supported'), ('and', 'cc', 'hotel'), ('the', 'det', 'casino'), ('casino', 'conj', 'hotel'), (';', 'punct', 'supported'), ('however', 'advmod', 'time'), (',', 'punct', 'however'), ('this', 'det', 'time'), ('time', 'npadvmod', 'supported'), ('...', 'punct', 'supported'), ('it', 'nsubj', 'was'), ('was', 'ROOT', 'was'), ('a', 'det', 'disaster'), ('disaster', 'attr', 'was'), ('.', 'punct', 'was'), ('Will', 'aux', 'go'), ('I', 'nsubj', 'go'), ('go', 'ROOT', 'go'), ('back', 'advmod', 'go'), (',', 'punct', 'go'), ('hopefullly', 'advmod', 'go'), (',', 'punct', 'go'), ('NO', 'npadvmod', 'go'), ('!', 'punct', 'go')]\n","\n","--- 5-STAR REVIEWS ---\n","\n","Original Text: Pandemic pit stop to have an ice cream.... only plain Sundae! Limited menu was written on the screens outside. So no unpleasant surprise.\n","Cashier was wearing gloves and mask, and him holding the item was good since he did not hold it with the lid. \n","There were only three customers at 8:15 pm.\n","Location is a bomb. Parking and access easy. Great visibility. \n","No pictures tonight.... soo\n","Sentences: ['Pandemic pit stop to have an ice cream....', 'only plain Sundae!', 'Limited menu was written on the screens outside.', 'So no unpleasant surprise.\\n', 'Cashier was wearing gloves and mask, and him holding the item was good since he did not hold it with the lid. \\n', 'There were only three customers at 8:15 pm.\\n', 'Location is a bomb.', 'Parking and access easy.', 'Great visibility. \\n', 'No pictures tonight....', 'soo']\n","Tokens: ['Pandemic', 'pit', 'stop', 'to', 'have', 'an', 'ice', 'cream', '....', 'only', 'plain', 'Sundae', '!', 'Limited', 'menu', 'was', 'written', 'on', 'the', 'screens', 'outside', '.', 'So', 'no', 'unpleasant', 'surprise', '.', '\\n', 'Cashier', 'was', 'wearing', 'gloves', 'and', 'mask', ',', 'and', 'him', 'holding', 'the', 'item', 'was', 'good', 'since', 'he', 'did', 'not', 'hold', 'it', 'with', 'the', 'lid', '.', '\\n', 'There', 'were', 'only', 'three', 'customers', 'at', '8:15', 'pm', '.', '\\n', 'Location', 'is', 'a', 'bomb', '.', 'Parking', 'and', 'access', 'easy', '.', 'Great', 'visibility', '.', '\\n', 'No', 'pictures', 'tonight', '....', 'soo']\n","POS Tags: [('Pandemic', 'ADJ'), ('pit', 'NOUN'), ('stop', 'NOUN'), ('to', 'PART'), ('have', 'VERB'), ('an', 'DET'), ('ice', 'NOUN'), ('cream', 'NOUN'), ('....', 'PUNCT'), ('only', 'ADV'), ('plain', 'ADJ'), ('Sundae', 'PROPN'), ('!', 'PUNCT'), ('Limited', 'PROPN'), ('menu', 'NOUN'), ('was', 'AUX'), ('written', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('screens', 'NOUN'), ('outside', 'ADV'), ('.', 'PUNCT'), ('So', 'ADV'), ('no', 'DET'), ('unpleasant', 'ADJ'), ('surprise', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Cashier', 'PROPN'), ('was', 'AUX'), ('wearing', 'VERB'), ('gloves', 'NOUN'), ('and', 'CCONJ'), ('mask', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('him', 'PRON'), ('holding', 'VERB'), ('the', 'DET'), ('item', 'NOUN'), ('was', 'AUX'), ('good', 'ADJ'), ('since', 'SCONJ'), ('he', 'PRON'), ('did', 'AUX'), ('not', 'PART'), ('hold', 'VERB'), ('it', 'PRON'), ('with', 'ADP'), ('the', 'DET'), ('lid', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('There', 'PRON'), ('were', 'VERB'), ('only', 'ADV'), ('three', 'NUM'), ('customers', 'NOUN'), ('at', 'ADP'), ('8:15', 'NUM'), ('pm', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('Location', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('bomb', 'NOUN'), ('.', 'PUNCT'), ('Parking', 'NOUN'), ('and', 'CCONJ'), ('access', 'NOUN'), ('easy', 'ADJ'), ('.', 'PUNCT'), ('Great', 'ADJ'), ('visibility', 'NOUN'), ('.', 'PUNCT'), ('\\n', 'SPACE'), ('No', 'DET'), ('pictures', 'NOUN'), ('tonight', 'NOUN'), ('....', 'PUNCT'), ('soo', 'PROPN')]\n","Lemmatized Tokens: ['pandemic', 'pit', 'stop', 'to', 'have', 'an', 'ice', 'cream', '....', 'only', 'plain', 'Sundae', '!', 'Limited', 'menu', 'be', 'write', 'on', 'the', 'screen', 'outside', '.', 'so', 'no', 'unpleasant', 'surprise', '.', '\\n', 'Cashier', 'be', 'wear', 'glove', 'and', 'mask', ',', 'and', 'he', 'hold', 'the', 'item', 'be', 'good', 'since', 'he', 'do', 'not', 'hold', 'it', 'with', 'the', 'lid', '.', '\\n', 'there', 'be', 'only', 'three', 'customer', 'at', '8:15', 'pm', '.', '\\n', 'Location', 'be', 'a', 'bomb', '.', 'parking', 'and', 'access', 'easy', '.', 'great', 'visibility', '.', '\\n', 'no', 'picture', 'tonight', '....', 'soo']\n","Named Entities: [('Sundae', 'GPE'), ('Cashier', 'PERSON'), ('only three', 'CARDINAL'), ('8:15 pm', 'TIME'), ('Location', 'ORG'), ('tonight', 'TIME')]\n","Dependency Parsing: [('Pandemic', 'amod', 'pit'), ('pit', 'compound', 'stop'), ('stop', 'ROOT', 'stop'), ('to', 'aux', 'have'), ('have', 'xcomp', 'stop'), ('an', 'det', 'cream'), ('ice', 'compound', 'cream'), ('cream', 'dobj', 'have'), ('....', 'punct', 'stop'), ('only', 'advmod', 'plain'), ('plain', 'ROOT', 'plain'), ('Sundae', 'npadvmod', 'plain'), ('!', 'punct', 'plain'), ('Limited', 'amod', 'menu'), ('menu', 'nsubjpass', 'written'), ('was', 'auxpass', 'written'), ('written', 'ROOT', 'written'), ('on', 'prep', 'written'), ('the', 'det', 'screens'), ('screens', 'pobj', 'on'), ('outside', 'advmod', 'screens'), ('.', 'punct', 'written'), ('So', 'advmod', 'surprise'), ('no', 'det', 'surprise'), ('unpleasant', 'amod', 'surprise'), ('surprise', 'ROOT', 'surprise'), ('.', 'punct', 'surprise'), ('\\n', 'dep', '.'), ('Cashier', 'nsubj', 'wearing'), ('was', 'aux', 'wearing'), ('wearing', 'ROOT', 'wearing'), ('gloves', 'dobj', 'wearing'), ('and', 'cc', 'gloves'), ('mask', 'conj', 'gloves'), (',', 'punct', 'wearing'), ('and', 'cc', 'wearing'), ('him', 'nsubj', 'was'), ('holding', 'acl', 'him'), ('the', 'det', 'item'), ('item', 'dobj', 'holding'), ('was', 'conj', 'wearing'), ('good', 'acomp', 'was'), ('since', 'mark', 'hold'), ('he', 'nsubj', 'hold'), ('did', 'aux', 'hold'), ('not', 'neg', 'hold'), ('hold', 'advcl', 'was'), ('it', 'dobj', 'hold'), ('with', 'prep', 'hold'), ('the', 'det', 'lid'), ('lid', 'pobj', 'with'), ('.', 'punct', 'was'), ('\\n', 'dep', '.'), ('There', 'expl', 'were'), ('were', 'ROOT', 'were'), ('only', 'advmod', 'three'), ('three', 'nummod', 'customers'), ('customers', 'attr', 'were'), ('at', 'prep', 'were'), ('8:15', 'nummod', 'pm'), ('pm', 'pobj', 'at'), ('.', 'punct', 'were'), ('\\n', 'dep', '.'), ('Location', 'nsubj', 'is'), ('is', 'ROOT', 'is'), ('a', 'det', 'bomb'), ('bomb', 'attr', 'is'), ('.', 'punct', 'is'), ('Parking', 'nsubj', 'easy'), ('and', 'cc', 'Parking'), ('access', 'conj', 'Parking'), ('easy', 'ROOT', 'easy'), ('.', 'punct', 'easy'), ('Great', 'amod', 'visibility'), ('visibility', 'ROOT', 'visibility'), ('.', 'punct', 'visibility'), ('\\n', 'dep', '.'), ('No', 'det', 'pictures'), ('pictures', 'ROOT', 'pictures'), ('tonight', 'npadvmod', 'pictures'), ('....', 'punct', 'pictures'), ('soo', 'ROOT', 'soo')]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-38a8984aba68>:31: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  five_star_reviews['processed'] = five_star_reviews['text'].apply(lambda x: nlp(x))\n"]}],"source":["# Define text processing functions\n","\n","def tokenize_and_segment(doc):\n","    \"\"\"Tokenize and segment sentences.\"\"\"\n","    sentences = [sent.text for sent in doc.sents]\n","    tokens = [token.text for token in doc]\n","    return sentences, tokens\n","\n","def pos_tagging(doc):\n","    \"\"\"Part-of-speech tagging.\"\"\"\n","    return [(token.text, token.pos_) for token in doc]\n","\n","def lemmatize(doc):\n","    \"\"\"Lemmatize the tokens.\"\"\"\n","    return [token.lemma_ for token in doc]\n","\n","def named_entity_recognition(doc):\n","    \"\"\"Extract named entities.\"\"\"\n","    return [(ent.text, ent.label_) for ent in doc.ents]\n","\n","def parse_dependencies(doc):\n","    \"\"\"Extract dependency relationships.\"\"\"\n","    return [(token.text, token.dep_, token.head.text) for token in doc]\n","\n","# Apply spaCy pipeline to reviews\n","one_star_reviews['processed'] = one_star_reviews['text'].apply(lambda x: nlp(x))\n","five_star_reviews['processed'] = five_star_reviews['text'].apply(lambda x: nlp(x))\n","\n","# Analyze each text processing technique\n","for review_category, reviews in zip(['1-star', '5-star'], [one_star_reviews, five_star_reviews]):\n","    print(f\"\\n--- {review_category.upper()} REVIEWS ---\\n\")\n","\n","    # Example review for demonstration\n","    example_doc = reviews['processed'].iloc[0]\n","    print(\"Original Text:\", example_doc.text)\n","\n","    # 1. Tokenization and Sentence Segmentation\n","    sentences, tokens = tokenize_and_segment(example_doc)\n","    print(\"Sentences:\", sentences)\n","    print(\"Tokens:\", tokens)\n","\n","    # 2. POS Tagging\n","    pos_tags = pos_tagging(example_doc)\n","    print(\"POS Tags:\", pos_tags)\n","\n","    # 3. Lemmatization\n","    lemmatized_tokens = lemmatize(example_doc)\n","    print(\"Lemmatized Tokens:\", lemmatized_tokens)\n","\n","    # 4. Named Entity Recognition (NER)\n","    entities = named_entity_recognition(example_doc)\n","    print(\"Named Entities:\", entities)\n","\n","    # 5. Dependency Parsing\n","    dependencies = parse_dependencies(example_doc)\n","    print(\"Dependency Parsing:\", dependencies)"]},{"cell_type":"code","execution_count":12,"id":"7494301f-7ce4-4079-aef4-6c9ec16cc2e4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7494301f-7ce4-4079-aef4-6c9ec16cc2e4","executionInfo":{"status":"ok","timestamp":1737949066034,"user_tz":420,"elapsed":225906,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"d7711dde-14f6-43ee-f44b-2b9b6fa2ebf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 20 nouns in 1-star reviews: [('food', 1754), ('order', 1040), ('time', 971), ('place', 875), ('service', 849), ('minutes', 678), ('restaurant', 590), ('manager', 430), ('people', 416), ('location', 381), ('customer', 374), ('chicken', 367), ('table', 333), ('experience', 288), ('pizza', 284), ('meal', 269), ('way', 260), ('staff', 259), ('money', 252), ('hour', 245)]\n","Top 20 nouns in 5-star reviews: [('food', 4736), ('place', 3141), ('service', 2128), ('time', 1714), ('restaurant', 1178), ('staff', 1124), ('pizza', 1019), ('menu', 947), ('chicken', 782), ('sauce', 672), ('experience', 634), ('meal', 606), ('order', 599), ('breakfast', 579), ('lunch', 573), ('spot', 547), ('cheese', 538), ('flavor', 531), ('town', 528), ('side', 524)]\n"]}],"source":["# Define a function to extract nouns\n","def extract_nouns(doc):\n","    return [token.text for token in nlp(doc) if token.pos_ == \"NOUN\"]\n","\n","# Extract nouns for each review category\n","nouns_1_star = [noun for review in one_star_reviews['text'] for noun in extract_nouns(review)]\n","nouns_5_star = [noun for review in five_star_reviews['text'] for noun in extract_nouns(review)]\n","\n","# Display top 20 nouns\n","print(\"Top 20 nouns in 1-star reviews:\", Counter(nouns_1_star).most_common(20))\n","print(\"Top 20 nouns in 5-star reviews:\", Counter(nouns_5_star).most_common(20))\n"]},{"cell_type":"code","execution_count":13,"id":"9fd8428a-167c-4142-a1f7-ea1e9b6aaf59","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9fd8428a-167c-4142-a1f7-ea1e9b6aaf59","executionInfo":{"status":"ok","timestamp":1737949327082,"user_tz":420,"elapsed":225717,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"0c59fdd7-8fd1-4c33-93e1-a34b48fcf2ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 20 adjectives in 1-star reviews: [('good', 479), ('other', 370), ('bad', 328), ('rude', 293), ('cold', 269), ('more', 248), ('last', 220), ('worst', 218), ('first', 214), ('better', 192), ('great', 187), ('wrong', 184), ('disappointed', 175), ('same', 170), ('terrible', 161), ('many', 159), ('horrible', 154), ('sure', 154), ('little', 150), ('old', 147)]\n","Top 20 adjectives in 5-star reviews: [('great', 2926), ('good', 2770), ('delicious', 1986), ('amazing', 1512), ('best', 1347), ('friendly', 1289), ('fresh', 971), ('Great', 941), ('nice', 919), ('more', 619), ('favorite', 616), ('perfect', 613), ('hot', 597), ('other', 592), ('little', 585), ('excellent', 584), ('first', 517), ('wonderful', 500), ('new', 498), ('tasty', 473)]\n"]}],"source":["# Define a function to extract adjectives\n","def extract_adjectives(doc):\n","    return [token.text for token in nlp(doc) if token.pos_ == \"ADJ\"]\n","\n","# Extract adjectives for each review category\n","adjectives_1_star = [adj for review in one_star_reviews['text'] for adj in extract_adjectives(review)]\n","adjectives_5_star = [adj for review in five_star_reviews['text'] for adj in extract_adjectives(review)]\n","\n","# Display top 20 adjectives\n","print(\"Top 20 adjectives in 1-star reviews:\", Counter(adjectives_1_star).most_common(20))\n","print(\"Top 20 adjectives in 5-star reviews:\", Counter(adjectives_5_star).most_common(20))\n"]},{"cell_type":"code","execution_count":14,"id":"5187a787-befa-4634-9f0a-419485a76690","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5187a787-befa-4634-9f0a-419485a76690","executionInfo":{"status":"ok","timestamp":1737949812357,"user_tz":420,"elapsed":235691,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"1d44232e-6d0f-442e-bcd1-a593a69b60dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 20 verbs in 1-star reviews: [('had', 1145), ('get', 790), ('have', 768), ('ordered', 710), ('go', 687), ('said', 620), ('got', 596), ('told', 558), ('asked', 537), ('came', 396), ('know', 386), ('went', 386), ('going', 346), ('give', 326), ('eat', 312), ('come', 310), ('took', 309), ('take', 307), ('made', 281), ('make', 278)]\n","Top 20 verbs in 5-star reviews: [('had', 3314), ('have', 1748), ('go', 1368), ('get', 1193), ('ordered', 1152), ('got', 1106), ('try', 964), ('recommend', 950), ('love', 937), ('made', 759), ('come', 728), ('came', 642), ('eat', 638), ('make', 559), ('tried', 546), ('order', 511), ('take', 510), ('has', 490), ('loved', 454), ('wait', 448)]\n"]}],"source":["# Define a function to extract verbs\n","def extract_verbs(doc):\n","    return [token.text for token in nlp(doc) if token.pos_ == \"VERB\"]\n","\n","# Extract verbs for each review category\n","verbs_1_star = [verb for review in one_star_reviews['text'] for verb in extract_verbs(review)]\n","verbs_5_star = [verb for review in five_star_reviews['text'] for verb in extract_verbs(review)]\n","\n","# Display top 20 verbs\n","print(\"Top 20 verbs in 1-star reviews:\", Counter(verbs_1_star).most_common(20))\n","print(\"Top 20 verbs in 5-star reviews:\", Counter(verbs_5_star).most_common(20))\n"]},{"cell_type":"code","execution_count":15,"id":"76735db0-4641-4ef5-8d53-17ab18ceb8c2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76735db0-4641-4ef5-8d53-17ab18ceb8c2","executionInfo":{"status":"ok","timestamp":1737950052130,"user_tz":420,"elapsed":234122,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"b9c85b99-9eea-4ffb-d335-1a1e10be6b6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Top 20 named entities in 1-star reviews: [('one', 267), ('two', 251), ('first', 245), ('2', 219), ('Tucson', 171), ('3', 163), ('today', 133), ('4', 104), ('5', 99), ('1', 96), ('three', 90), ('half', 82), ('First', 79), ('second', 77), ('10', 76), ('Mexican', 72), ('tonight', 67), ('20', 58), ('20 minutes', 58), ('zero', 57)]\n","Top 20 named entities in 5-star reviews: [('Tucson', 1478), ('first', 552), ('Mexican', 418), ('two', 402), ('one', 385), ('5', 249), ('2', 221), ('One', 209), ('today', 192), ('First', 173), ('Chinese', 171), ('3', 167), ('Italian', 132), ('Love', 119), ('half', 113), ('4', 110), ('French', 104), ('Arizona', 103), ('second', 100), ('three', 100)]\n"]}],"source":["# Define a function to extract named entities\n","def extract_named_entities(doc):\n","    entities = [ent.text for ent in nlp(doc).ents]\n","    return entities\n","\n","# Extract named entities for each review category\n","entities_1_star = [entity for review in one_star_reviews['text'] for entity in extract_named_entities(review)]\n","entities_5_star = [entity for review in five_star_reviews['text'] for entity in extract_named_entities(review)]\n","\n","# Display top 20 named entities\n","print(\"Top 20 named entities in 1-star reviews:\", Counter(entities_1_star).most_common(20))\n","print(\"Top 20 named entities in 5-star reviews:\", Counter(entities_5_star).most_common(20))\n"]},{"cell_type":"markdown","id":"a8f87e85-6c86-4136-8271-e30cd8df48ae","metadata":{"id":"a8f87e85-6c86-4136-8271-e30cd8df48ae"},"source":["The language used in 1-star reviews frequently includes negative nouns like \"waiter,\" \"service,\" or \"manager\" and adjectives such as \"terrible\" or \"slow.\" Positive reviews often contain nouns like \"food,\" \"experience,\" or \"meal\" and adjectives like \"amazing\" or \"delicious.\" Good restaurant experiences revolve around high food quality, excellent service, and a pleasant atmosphere, as evident in the 5-star reviews."]},{"cell_type":"markdown","id":"c2b63338-d983-43f7-8713-6467bce52e57","metadata":{"id":"c2b63338-d983-43f7-8713-6467bce52e57"},"source":["I used GenAI tools to guide the structure of the assignment but wrote and implemented all the code independently. I did not collaborate with others on this assignment."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AugLI_d04dfv","executionInfo":{"status":"ok","timestamp":1737950138743,"user_tz":420,"elapsed":22825,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"0d0d90fa-8e0a-439d-ff38-eac58a43b1de"},"id":"AugLI_d04dfv","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":18,"id":"52ce8684-c593-459d-b220-f2f848844148","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52ce8684-c593-459d-b220-f2f848844148","executionInfo":{"status":"ok","timestamp":1737950191515,"user_tz":420,"elapsed":2644,"user":{"displayName":"Ujwala Jayarama","userId":"17835069501283601688"}},"outputId":"96064d21-e534-462c-ae74-f93d2414e17b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] WARNING | pattern '/content/drive/MyDrive/Topic' matched no files\n","[NbConvertApp] WARNING | pattern 'Modelling/LA1_Jayatama_UjwalaKavya.ipynb' matched no files\n","This application is used to convert notebook files (*.ipynb)\n","        to various other formats.\n","\n","        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n","\n","Options\n","=======\n","The options below are convenience aliases to configurable class-options,\n","as listed in the \"Equivalent to\" description-line of the aliases.\n","To see all configurable class-options for some <cmd>, use:\n","    <cmd> --help-all\n","\n","--debug\n","    set log level to logging.DEBUG (maximize logging output)\n","    Equivalent to: [--Application.log_level=10]\n","--show-config\n","    Show the application's configuration (human-readable format)\n","    Equivalent to: [--Application.show_config=True]\n","--show-config-json\n","    Show the application's configuration (json format)\n","    Equivalent to: [--Application.show_config_json=True]\n","--generate-config\n","    generate default config file\n","    Equivalent to: [--JupyterApp.generate_config=True]\n","-y\n","    Answer yes to any questions instead of prompting.\n","    Equivalent to: [--JupyterApp.answer_yes=True]\n","--execute\n","    Execute the notebook prior to export.\n","    Equivalent to: [--ExecutePreprocessor.enabled=True]\n","--allow-errors\n","    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n","    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n","--stdin\n","    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n","    Equivalent to: [--NbConvertApp.from_stdin=True]\n","--stdout\n","    Write notebook output to stdout instead of files.\n","    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n","--inplace\n","    Run nbconvert in place, overwriting the existing notebook (only\n","            relevant when converting to notebook format)\n","    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n","--clear-output\n","    Clear output of current file and save in place,\n","            overwriting the existing notebook.\n","    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n","--coalesce-streams\n","    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n","    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n","--no-prompt\n","    Exclude input and output prompts from converted document.\n","    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n","--no-input\n","    Exclude input cells and output prompts from converted document.\n","            This mode is ideal for generating code-free reports.\n","    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n","--allow-chromium-download\n","    Whether to allow downloading chromium if no suitable version is found on the system.\n","    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n","--disable-chromium-sandbox\n","    Disable chromium security sandbox when converting to PDF..\n","    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n","--show-input\n","    Shows code input. This flag is only useful for dejavu users.\n","    Equivalent to: [--TemplateExporter.exclude_input=False]\n","--embed-images\n","    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n","    Equivalent to: [--HTMLExporter.embed_images=True]\n","--sanitize-html\n","    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n","    Equivalent to: [--HTMLExporter.sanitize_html=True]\n","--log-level=<Enum>\n","    Set the log level by value or name.\n","    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n","    Default: 30\n","    Equivalent to: [--Application.log_level]\n","--config=<Unicode>\n","    Full path of a config file.\n","    Default: ''\n","    Equivalent to: [--JupyterApp.config_file]\n","--to=<Unicode>\n","    The export format to be used, either one of the built-in formats\n","            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n","            or a dotted object name that represents the import path for an\n","            ``Exporter`` class\n","    Default: ''\n","    Equivalent to: [--NbConvertApp.export_format]\n","--template=<Unicode>\n","    Name of the template to use\n","    Default: ''\n","    Equivalent to: [--TemplateExporter.template_name]\n","--template-file=<Unicode>\n","    Name of the template file to use\n","    Default: None\n","    Equivalent to: [--TemplateExporter.template_file]\n","--theme=<Unicode>\n","    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n","    as prebuilt extension for the lab template)\n","    Default: 'light'\n","    Equivalent to: [--HTMLExporter.theme]\n","--sanitize_html=<Bool>\n","    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n","    should be set to True by nbviewer or similar tools.\n","    Default: False\n","    Equivalent to: [--HTMLExporter.sanitize_html]\n","--writer=<DottedObjectName>\n","    Writer class used to write the\n","                                        results of the conversion\n","    Default: 'FilesWriter'\n","    Equivalent to: [--NbConvertApp.writer_class]\n","--post=<DottedOrNone>\n","    PostProcessor class used to write the\n","                                        results of the conversion\n","    Default: ''\n","    Equivalent to: [--NbConvertApp.postprocessor_class]\n","--output=<Unicode>\n","    Overwrite base name use for output files.\n","                Supports pattern replacements '{notebook_name}'.\n","    Default: '{notebook_name}'\n","    Equivalent to: [--NbConvertApp.output_base]\n","--output-dir=<Unicode>\n","    Directory to write output(s) to. Defaults\n","                                  to output to the directory of each notebook. To recover\n","                                  previous default behaviour (outputting to the current\n","                                  working directory) use . as the flag value.\n","    Default: ''\n","    Equivalent to: [--FilesWriter.build_directory]\n","--reveal-prefix=<Unicode>\n","    The URL prefix for reveal.js (version 3.x).\n","            This defaults to the reveal CDN, but can be any url pointing to a copy\n","            of reveal.js.\n","            For speaker notes to work, this must be a relative path to a local\n","            copy of reveal.js: e.g., \"reveal.js\".\n","            If a relative path is given, it must be a subdirectory of the\n","            current directory (from which the server is run).\n","            See the usage documentation\n","            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n","            for more details.\n","    Default: ''\n","    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n","--nbformat=<Enum>\n","    The nbformat version to write.\n","            Use this to downgrade notebooks.\n","    Choices: any of [1, 2, 3, 4]\n","    Default: 4\n","    Equivalent to: [--NotebookExporter.nbformat_version]\n","\n","Examples\n","--------\n","\n","    The simplest way to use nbconvert is\n","\n","            > jupyter nbconvert mynotebook.ipynb --to html\n","\n","            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n","\n","            > jupyter nbconvert --to latex mynotebook.ipynb\n","\n","            Both HTML and LaTeX support multiple output templates. LaTeX includes\n","            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n","            'classic'. You can specify the flavor of the format used.\n","\n","            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n","\n","            You can also pipe the output to stdout, rather than a file\n","\n","            > jupyter nbconvert mynotebook.ipynb --stdout\n","\n","            PDF is generated via latex\n","\n","            > jupyter nbconvert mynotebook.ipynb --to pdf\n","\n","            You can get (and serve) a Reveal.js-powered slideshow\n","\n","            > jupyter nbconvert myslides.ipynb --to slides --post serve\n","\n","            Multiple notebooks can be given at the command line in a couple of\n","            different ways:\n","\n","            > jupyter nbconvert notebook*.ipynb\n","            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n","\n","            or you can specify the notebooks list in a config file, containing::\n","\n","                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n","\n","            > jupyter nbconvert --config mycfg.py\n","\n","To see all available configurables, use `--help-all`.\n","\n"]}],"source":["!jupyter nbconvert --to html /content/drive/MyDrive/Topic Modelling/LA1_Jayatama_UjwalaKavya.ipynb"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}
